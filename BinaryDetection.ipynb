{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How likely is it to detect a certain Cepheid?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this short notebook, I want to calculate how likely it is that orbital motion of a Cepheid can be detected in radial velocity (RV) measurements, given an observational uncertainty and an observing sequence.\n",
    "Clearly, it is easier to see the RV modulation if the companion is more massive and the orbit is shorter, but even in the best case, there is still a certain probability that no RV varibility is detected, because the orbital plane conincides with the plane of the sky.\n",
    "\n",
    "In this notebook, I keep the code for the calculation together with some plots that illustrate the results and some notes to help you understand what I did and to help me remember how I did it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import python modules.\n",
    "from __future__ import division\n",
    "from itertools import izip\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as u\n",
    "import astropy.table as table\n",
    "import astropy.io.ascii as ascii\n",
    "from PyAstronomy.pyasl import KeplerEllipse\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed magnitude of the RV variation depends crucially on the angle between the line-of-sight (LOS) and the orbit, particularly the inclination and the longitude of periastron. They way I handle that here is that for each orbit I calculate the observed RV values for a large number of sightlines. I then check for which fraction of those sightlines the RV change is large enough to be detected. \n",
    "To obtain a fair estimate of the probability of detection I need an isotropic distribution of sightlines. There are schemes in the literature to tesselate a sphere in such a way that each tesselate has the same area (\"geodesic grid\") and then I could use the mid-point of this tesselate for my sightlines. However, all those schemes are a pain to implement (or if implemented already, a pain to use) due to all the sin and cos involved. It's not impossible, but for now I just generate a large number of random LOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_LOSs(n):\n",
    "    '''Generate a random distribution of Line of sights\n",
    "    \n",
    "    Generate a random distribution of sightlines in cartesian coordinates,\n",
    "    with an isotropic distribution (no clustering around the poles).\n",
    "    Analytically, a truely isotropic distribution could be generated using\n",
    "    geodesic grids, but this is much faster to program and (for large n) just as good.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : integer\n",
    "        number of sightlines to be generated\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    los : np.array of dim [3,m] with m < n\n",
    "        x,y,z cartesian coordinates for each sightline. Only a subset of all generated\n",
    "        sightlines is returend, selected in such a way that they are (within the limits\n",
    "        of their randomness) isotropically distributed over the sphere.\n",
    "    '''\n",
    "    # Make points in box with x,y,z in [-1,1].\n",
    "    los = np.random.random((3, n)) * 2 - 1.\n",
    "    # Throw out all points beyond a unit sphere.\n",
    "    # Yes, this is a wasteful implementation, but it's fast enough that I don't care.\n",
    "    r = np.sqrt((los * los).sum(axis=0))\n",
    "    ind = ((r <= 1) & (r >= 1e-6))  # Throw out inner values where round-off errors matter.\n",
    "    return los[:, ind] / r[ind]\n",
    "\n",
    "def get_LOSs(n = 1000):\n",
    "    '''Generate a random distribution of Line of sights\n",
    "    \n",
    "    Generate a random distribution of sightlines in cartesian coordinates,\n",
    "    with an isotropic distribution (no clustering around the poles).\n",
    "    Analytically, a truely isotropic distribution could be generated using\n",
    "    geodesic grids, but this is much faster to program and (for large n) just as good.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : integer\n",
    "        number of sightlines to be generated\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    los : np.array of dim [3,m] with m < n\n",
    "        x,y,z cartesian coordinates for each sightline. \n",
    "    '''\n",
    "    while True:\n",
    "        los = generate_LOSs(4 * n) # 2 should be big enough that this usually succeeds.\n",
    "        if los.shape[1] >= n:\n",
    "            break\n",
    "    return los[:, :n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_many_vobs(times, a, p, e, n_tau=50, n_los=1000, los=None):\n",
    "    '''Calculate radial velocities for one orbit and many LOS.\n",
    "    \n",
    "    For one orbit with given semi-major axis, period, and eccentricity calculate\n",
    "    the radial velocity (RV) signal for a given set of observaton times. \n",
    "    This calculation is done for larger number of sightlines to the system and for\n",
    "    different starting points in the orbit.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : float\n",
    "        semi-major axis (in AU)\n",
    "    p : float\n",
    "        period (in years)\n",
    "    e : float\n",
    "        eccentricity of orbit\n",
    "    n_tau : integer\n",
    "        The calculation will be done for ``n_tau`` different stating points\n",
    "        regularly distributed over the orbit, because it does matter if a \n",
    "        star is observed close to periastron or apastron.\n",
    "    n_los : integer\n",
    "        number of lines-of-sight to be evaluated\n",
    "    los : None or np.array of dim [3, n]\n",
    "        If ``None`` then ``n_los`` sightlines will be randomnly generated.\n",
    "        Otherwise a defined set of sightlines can be passed in as an array.\n",
    "        The elements of the array have to be the cartesian coordinates of \n",
    "        points on the unit sphere.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    v_obs : array of astropy.quantity with dim [n_tau, len(times), n_los]\n",
    "        This holds the calculated RV values that would be observed.\n",
    "    ''' \n",
    "    \n",
    "    if los is None:\n",
    "        los = get_LOSs(n_los)\n",
    "    else:\n",
    "        n_los = los.shape[1]\n",
    "        \n",
    "    taus = np.linspace(0,p, n_tau, endpoint=False)\n",
    "    v_obs = np.zeros((n_tau, len(times), n_los))\n",
    "    for j, tau in enumerate(taus):\n",
    "        ke = KeplerEllipse(a, p, e, tau)\n",
    "        vxyz = ke.xyzVel(times)\n",
    "        for i in range(len(times)):\n",
    "            v_obs[j, i,:] = np.dot(vxyz[i,:], los)\n",
    "\n",
    "    return v_obs *u.AU / u.year\n",
    "\n",
    "def calc_maxdv(*args, **kwargs):\n",
    "    '''Run ``calc_many_vobs`` and simplyfy its output.\n",
    "    \n",
    "    See ``calc_many_vobs`` for accepted parameters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    maxdv : astropy.quantity\n",
    "        Maximum differences between two RV values that would be \n",
    "        observed for the given orbital parameters for each LOS and tau.\n",
    "    '''\n",
    "    v_obs = calc_many_vobs(*args, **kwargs)\n",
    "    maxdv = np.max(v_obs, axis=1) - np.min(v_obs, axis=1)\n",
    "    return maxdv.flatten()\n",
    "\n",
    "def prob_to_detect(dv_sensitivity, *args, **kwargs):\n",
    "    '''Calculate the probability to detect binarity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dv_sensitivity : astropy.quantity\n",
    "        Minimum delta(RV) required for detection.\n",
    "        \n",
    "    See ``calc_many_vobs`` for the remaining  parameters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    prob : float\n",
    "        Probability of detection\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    \n",
    "    >>> times = np.arange(0,1,.1)\n",
    "    >>> prob_to_detect((20.*u.km/u.s), times, 1 , 1, 0)\n",
    "    0.94\n",
    "    (approximate result, since based on random numbers)\n",
    "    '''\n",
    "    maxdv = calc_maxdv(*args, **kwargs)\n",
    "    return (maxdv > dv_sensitivity).sum() / maxdv.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the grid of parameters. I chose the secondary mass $m_2$ and the total semi-major axis $a$ as parameters. From this I calculate the semi-major axis of the primary and the period assuming a constant primary mass of $m_1 = 6 M_{\\odot}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Observation cadence. Here: 10 years, once per year, regular intervals.\n",
    "times = np.arange(0.,10.,1.)\n",
    "\n",
    "# Grid for semi-major axis between the two stars (in AU)\n",
    "a = np.logspace(0.1,2)\n",
    "# Masses (in M_sun)\n",
    "m1 = 5   # primary\n",
    "m2 = np.array([0.02, 0.05, 0.08, 0.1,0.5, 0.8,1,2,3,4,5])  # secondary\n",
    "M = m1 + m2  # total mass of system\n",
    "agrid, m2grid = np.meshgrid(a, m2)\n",
    "# semi-major axis of primary\n",
    "a1grid = agrid * m2grid / (m1 + m2grid)\n",
    "# Period\n",
    "Pgrid = (m1 + m2grid) * np.sqrt(a1grid**3 / m2grid**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "cs = ax1.contour(Pgrid, [0.1,0.2,1,5,25,50,100,300])\n",
    "ax1.clabel(cs)\n",
    "ax1.set_title('Period grid [yr]')\n",
    "cs = ax2.contour(m2grid/m1)\n",
    "ax2.clabel(cs)\n",
    "ax2.set_title('mass ratio grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first plot shows the orbital period in years. This shall serve as a comparison for the following plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cs = plt.contour(agrid, m2grid, Pgrid, [10,20,30,60,120,240.])\n",
    "plt.clabel(cs)\n",
    "plt.xlabel('Semi-major axis [AU]')\n",
    "plt.ylabel('mass of secondary [$M_{\\odot}$]')\n",
    "plt.title('Orbital period [years]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I show the probability of detection for circular orbits. The plots on the left and on the right show the same data, but one has a linear and one has logarithmic axis. Both of them are a little coarse, because the grid is not very dense. That can easily be changed if we want to make nicer looking plots.\n",
    "\n",
    "On the right plot, there are islands where the chance to detect a binary is essentially 0. This happens when the sampling frequency is (or is close to) an integer multiple of the orbital period. Again, this will look better (and more regular) when I use a denser grid.\n",
    "\n",
    "Those thoughts aside, we see that we expect to detect almost every solar-mass or heavier companion out to 20 AU, and have a chance of 50% out to about 40 AU.\n",
    "\n",
    "For all plots I use 2 km/s as minimum $\\Delta v$ that would be detected in the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prop_e0 = np.zeros_like(Pgrid)\n",
    "for x, y in np.ndindex(Pgrid.shape):\n",
    "    prop_e0[x,y] = prob_to_detect(2.*u.km/u.s, times, a1grid[x,y], Pgrid[x,y], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_m_x(array):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    for ax in [ax1, ax2]:\n",
    "        cs = ax.contour(agrid, m2grid, array)\n",
    "        ax.clabel(cs)\n",
    "        ax.set_xlabel('Semi-major axis [AU]')\n",
    "        ax.set_ylabel('mass of secondary [$M_{\\odot}$]')\n",
    "        ax.set_title('Probability of detection for $e$ = 0')\n",
    "    ax2.set_title('Same data, logarithmic axes')\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "    return fig\n",
    "\n",
    "def plot_q_p(array):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    for ax in [ax1, ax2]:\n",
    "        cs = ax.contour(Pgrid, m2grid/m1, array)\n",
    "        ax.clabel(cs)\n",
    "        ax.set_xlabel('Period [yr]')\n",
    "        ax.set_ylabel('mass ratio [$M_{\\odot}$]')\n",
    "        ax.set_title('Probability of detection for $e$ = 0')\n",
    "    ax2.set_title('Same data, logarithmic axes')\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plot_m_x(prop_e0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Same plot as above, but with different axis\n",
    "\n",
    "fig = plot_q_p(prop_e0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we repeat the excersice for $e=0.5$. As you can see the probability contours move inwards a little, but overall that is not a dramatic change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times = np.arange(0.,10.,1.)\n",
    "prop_e05 = np.zeros_like(Pgrid)\n",
    "for x, y in np.ndindex(Pgrid.shape):\n",
    "    prop_e05[x,y] = prob_to_detect(2.*u.km/u.s, times, a1grid[x,y], Pgrid[x,y], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cs = plt.contour(agrid, m2grid, prop_e05)\n",
    "plt.clabel(cs)\n",
    "plt.xlabel('Semi-major axis [AU]')\n",
    "plt.ylabel('mass of secondary [$M_{\\odot}$]')\n",
    "plt.title('Probability of detection for $e$ = 0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last point, I use circular orbits again to predict how much better this will be with a 20 year baseline. You can use that to predict the number of additional binary systems that will be identified when new data becomes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times = np.arange(0.,20.,1.)\n",
    "prop_e05 = np.zeros_like(Pgrid)\n",
    "for x, y in np.ndindex(Pgrid.shape):\n",
    "    prop_e05[x,y] = prob_to_detect(2.*u.km/u.s, times, a1grid[x,y], Pgrid[x,y], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cs = plt.contour(agrid, m2grid, prop_e05)\n",
    "plt.clabel(cs)\n",
    "plt.xlabel('Semi-major axis [AU]')\n",
    "plt.ylabel('mass of secondary [$M_{\\odot}$]')\n",
    "plt.title('Probability of detection for $e$ = 0 with 20 year baseline' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run simulations with the actual observing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actual_obs_years = table.Table.read('datafile4.txt', format='ascii')\n",
    "actual_obs_years = actual_obs_years.filled()\n",
    "actual_obs_years = actual_obs_years.group_by('ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just do this for eccentricity 0 and 0.5. Since there is little prior information and (see above) it also does not matter much in that range, we will just show that the contours are very similar and no more details are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allsims00 = {}\n",
    "for star, group in izip(actual_obs_years.groups.keys, actual_obs_years.groups):\n",
    "    print datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'), 'Working on: ', star[0]\n",
    "    prop = np.zeros_like(Pgrid)\n",
    "    for x, y in np.ndindex(Pgrid.shape):\n",
    "        prop[x,y] = prob_to_detect(2.*u.km/u.s, np.array(group['Year']), a1grid[x,y], Pgrid[x,y], 0.0)\n",
    "    allsims00[star[0]] = prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allsims05 = {}\n",
    "for star, group in izip(actual_obs_years.groups.keys, actual_obs_years.groups):\n",
    "    print datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'), 'Working on: ', star[0]\n",
    "    prop = np.zeros_like(Pgrid)\n",
    "    for x, y in np.ndindex(Pgrid.shape):\n",
    "        prop[x, y] = prob_to_detect(2. * u.km/u.s, np.array(group['Year']), a1grid[x, y], Pgrid[x, y], 0.5)\n",
    "    allsims05[star[0]] = prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allnames = set(allsims00.keys()) - set(['CK  Cam'])\n",
    "all00 = np.dstack([allsims00[n] for n in allnames]).mean(axis=2)\n",
    "all05 = np.dstack([allsims05[n] for n in allnames]).mean(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plot_q_p(allsims00[list(allnames)[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plot_q_p(all00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How dense are the grids in period and mass ratio space? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "cs = ax1.contour(Pgrid, [0.1,0.2,1,5,25,50,100,300])\n",
    "ax1.clabel(cs)\n",
    "ax1.set_title('Period grid')\n",
    "cs = ax2.contour(m2grid/m1)\n",
    "ax2.clabel(cs)\n",
    "ax2.set_title('mass ratio grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.min(Pgrid), np.max(Pgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the figures for publication together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "ax = fig.add_subplot(111)\n",
    "cs = ax.contour(Pgrid, m2grid/m1, all00, [0.95, 0.90, 0.75], linewidths=4, linestyles=['solid', 'dashed','dotted'])\n",
    "cs2 = ax.contour(Pgrid, m2grid/m1, all05, [0.95, 0.90, 0.75], linestyles=['solid', 'dashed','dotted'])\n",
    "ax.set_xlim([0,60])\n",
    "ax.clabel(cs, fmt='%1.2f', use_clabeltext=True, manual=[(40,.4), (30,.6), (20,.5)])\n",
    "\n",
    "ax.set_xlabel('Period [yr]')\n",
    "ax.set_ylabel('mass ratio')\n",
    "\n",
    "#ax.set_xscale(\"log\")\n",
    "fig.subplots_adjust(left=0.16, bottom=0.16, top=0.97, right=0.97)\n",
    "fig.savefig('detectionprobability.png')\n",
    "fig.savefig('detectionprobability.pdf')\n",
    "fig.savefig('detectionprobability.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Monte-Carlo Simulations to estimate the fraction of Cepheid compagnions that will be detected with our method. For each star in our sample we perform the following simulation: We assume a Cepheid mass of $5\\;M_{\\odot}$ and form a grid of eleven values for the mass of the secondary and 50 logarithmically spaced values for the semi-major axis corresponding to periods between 0.5 and 500 years. For each grid point, we generate an array of 1000 random lines-of-sight to the system and for each line-of-sight we calculate the radial velocity of the Cepheid at the same time cadence that is shown for the star in question in Table~\\ref{tab:thetableyousendme}. For systems with a large semi-major axis the orbital period of the system can be much longer than the sequence covered by the observations. In this case, the initial position of the secondary becomes important, since the radial velocity of the Cepheid changes much faster close to Periastron than at Apastron. Thus, we repeat the simulations for 50 evenly spaced initial positions on an elliptical orbit. We then calculate which fraction of the total 50000 simulations for each grid point predicts a velocity difference between the highst and lowest radial velocity $>2$~km~s$^{-1}$. We take this as an estimate of the probability to detect a compagnion with these system parameters based on the radial velocity of the Cepheid.\n",
    "\n",
    "The Kepler equations for the orbit are solved using the implementation of PyAstronomy \\footnote{http://www.hs.uni-hamburg.de/DE/Ins/Per/Czesla/PyA/PyA/index.html} that is based on the algorithm of \\citet{1995CeMDA..63..101M}. The code for our Monte-Carlo simulations is implemented as an IPython notebook \\citep{IPython}, which is available at https://github.com/hamogu/Cepheids.\n",
    "\n",
    "\\begin{figure}\n",
    "\\plotone{detectionprobability}\n",
    "\\caption{\\label{fig:detectionprobability} Probability to detect the binary compagnion of a Cepheid based on the radial velocity of the primary. The simulations assume that a radial velocity $> 2$~km~s$^{-1}$ leads to a significant detection. Further assumptions are discussed in the text. The thick lines are contours of the detection probability for circular orbits, the thin lines for elliptical orbits with $\\epsilon=0.5$. The value of the detection probability for the circular orbits is labelled in the plot, they also apply to the thin liens of equal color and line style.}\n",
    "\\end{figure}\n",
    "\n",
    "We perform this whole set of simulations for each star in our sample twice. Once we assume circular orbits and the other time elliptical orbits with $\\epsilon=0.5$. For given binary parameters, the probability to detect a compagnion varies between different stars in our sample, because they are observed on different time cadences. We thus average the results over the entire sample. Figure~\\ref{fig:detectionprobability} shows a countour map of the averaged detection probabilities. It shows a lack of sensitivity to periods below about one year, because the radial velocities used in this study are averaged on an annual basis (Table~\\ref{tab:thetableyousendme}). Except for very low mass compagnions, we expect to find almost all binary systems with periods below 10 or twenty years, where higher mass secondaries allow a detection for larger semi-major axes and thus longer periods. For a mass ratio $q=0.4$ we still expect to detect three quarters of all binary systems with circular orbits out to periods of 40~yr. The numbers are a little lower for ellipical orbits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@ARTICLE{1995CeMDA..63..101M,\n",
    "   author = {{Markley}, F.~L.},\n",
    "    title = \"{Kepler Equation Solver}\",\n",
    "  journal = {Celestial Mechanics and Dynamical Astronomy},\n",
    " keywords = {Kepler's Equation, two-body problem, elliptical motion, numerical methods, orbit propagation},\n",
    "     year = 1995,\n",
    "    month = mar,\n",
    "   volume = 63,\n",
    "    pages = {101-111},\n",
    "      doi = {10.1007/BF00691917},\n",
    "   adsurl = {http://adsabs.harvard.edu/abs/1995CeMDA..63..101M},\n",
    "  adsnote = {Provided by the SAO/NASA Astrophysics Data System}\n",
    "}\n",
    "@Article{IPython,\n",
    "  Author    = {P\\'erez, Fernando and Granger, Brian E.},\n",
    "  Title     = {{IP}ython: a System for Interactive Scientific Computing},\n",
    "  Journal   = {Computing in Science and Engineering},\n",
    "  Volume    = {9},\n",
    "  Number    = {3},\n",
    "  Pages     = {21--29},\n",
    "  month     = may,\n",
    "  year      = 2007,\n",
    "  url       = \"http://ipython.org\",\n",
    "  ISSN      = \"1521-9615\",\n",
    "  doi       = {10.1109/MCSE.2007.53},\n",
    "  publisher = {IEEE Computer Society},\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
